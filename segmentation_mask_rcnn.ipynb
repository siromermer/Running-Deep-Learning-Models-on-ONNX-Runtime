{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acc3f0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "cuDNN version: 91002\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "983c4836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.22.0\n",
      "GPU\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "\n",
    "print(onnxruntime.__version__)\n",
    "print(onnxruntime.get_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b668e13e",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1412176",
   "metadata": {},
   "source": [
    "# Instance Segmentation with Mask R-CNN and ONNX Runtime GPU\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Export a pre-trained Mask R-CNN model to ONNX format\n",
    "2. Run the model on video using ONNX Runtime with GPU acceleration\n",
    "3. Visualize instance segmentation results with colored masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cb2387",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69401a8",
   "metadata": {},
   "source": [
    "## 1. Export Mask R-CNN model to ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b339400b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omer/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/omer/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/tmp/ipykernel_6664/1676289399.py:16: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n",
      "/home/omer/.local/lib/python3.10/site-packages/torch/nn/functional.py:4705: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  * torch.tensor(scale_factors[i], dtype=torch.float32)\n",
      "/home/omer/.local/lib/python3.10/site-packages/torchvision/ops/boxes.py:174: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  boxes_x = torch.min(boxes_x, torch.tensor(width, dtype=boxes.dtype, device=boxes.device))\n",
      "/home/omer/.local/lib/python3.10/site-packages/torchvision/ops/boxes.py:176: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  boxes_y = torch.min(boxes_y, torch.tensor(height, dtype=boxes.dtype, device=boxes.device))\n",
      "/home/omer/.local/lib/python3.10/site-packages/torch/__init__.py:2174: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert condition, message\n",
      "/home/omer/.local/lib/python3.10/site-packages/torchvision/models/detection/transform.py:308: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(s, dtype=torch.float32, device=boxes.device)\n",
      "/home/omer/.local/lib/python3.10/site-packages/torchvision/models/detection/transform.py:309: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  / torch.tensor(s_orig, dtype=torch.float32, device=boxes.device)\n",
      "/home/omer/.local/lib/python3.10/site-packages/torchvision/models/detection/roi_heads.py:392: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(M + 2 * padding).to(torch.float32) / torch.tensor(M).to(torch.float32)\n",
      "/home/omer/.local/lib/python3.10/site-packages/torch/onnx/symbolic_opset9.py:5350: UserWarning: Exporting aten::index operator of advanced indexing in opset 11 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n",
      "/home/omer/.local/lib/python3.10/site-packages/torch/onnx/_internal/jit_utils.py:309: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:178.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/home/omer/.local/lib/python3.10/site-packages/torch/onnx/utils.py:684: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:178.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/home/omer/.local/lib/python3.10/site-packages/torch/onnx/utils.py:1154: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:178.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask R-CNN model exported to models/maskrcnn.onnx with input size 800x800\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# Define model input dimensions\n",
    "model_width = 800\n",
    "model_height = 800\n",
    "\n",
    "# Load pretrained Mask R-CNN model\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Create dummy input - typical input size for Mask R-CNN\n",
    "dummy_input = torch.randn(1, 3, model_height, model_width)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    dummy_input, \n",
    "    \"models/maskrcnn.onnx\",\n",
    "    opset_version=11,\n",
    "    input_names=[\"images\"],\n",
    "    output_names=[\"boxes\", \"labels\", \"scores\", \"masks\"],\n",
    "    dynamic_axes={\n",
    "        \"images\": {0: \"batch_size\"},\n",
    "        \"boxes\": {0: \"num_detections\"},\n",
    "        \"labels\": {0: \"num_detections\"},\n",
    "        \"scores\": {0: \"num_detections\"},\n",
    "        \"masks\": {0: \"num_detections\"}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Mask R-CNN model exported to models/maskrcnn.onnx with input size {model_width}x{model_height}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc051fc1",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d235c69c",
   "metadata": {},
   "source": [
    "## 2. Run Mask R-CNN model on video with ONNX Runtime GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85baef88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available ONNX Runtime providers: ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "CUDA provider available: True\n",
      "Session providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Input name: images\n",
      "Output 0: boxes, shape: ['num_detections', 4]\n",
      "Output 1: labels, shape: ['num_detections']\n",
      "Output 2: scores, shape: ['num_detections']\n",
      "Output 3: masks, shape: ['num_detections', 1, 'Unsqueezemasks_dim_2', 'Unsqueezemasks_dim_3']\n",
      "Loaded 81 class names\n",
      "Using model input dimensions: 800x800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-08-24 16:21:43.598821202 [W:onnxruntime:, transformer_memcpy.cc:83 ApplyImpl] 2 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2025-08-24 16:21:43.599753135 [W:onnxruntime:, transformer_memcpy.cc:83 ApplyImpl] 12 Memcpy nodes are added to the graph sub_graph4 for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2025-08-24 16:21:43.605679264 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-08-24 16:21:43.605699447 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Check available providers first\n",
    "print(\"Available ONNX Runtime providers:\", ort.get_available_providers())\n",
    "print(\"CUDA provider available:\", 'CUDAExecutionProvider' in ort.get_available_providers())\n",
    "\n",
    "# Load pretrained Mask R-CNN model\n",
    "session = ort.InferenceSession(\"models/maskrcnn.onnx\", providers=['CUDAExecutionProvider'])\n",
    "print(\"Session providers:\", session.get_providers())\n",
    "\n",
    "# Get input details\n",
    "input_details = session.get_inputs()[0]\n",
    "print(f\"Input name: {input_details.name}\")\n",
    "\n",
    "# Check output shapes\n",
    "for i, output in enumerate(session.get_outputs()):\n",
    "    print(f\"Output {i}: {output.name}, shape: {output.shape}\")\n",
    "\n",
    "# Load COCO class names from file\n",
    "with open('resources/coco_labels_rcnn.txt', 'r') as f:\n",
    "    coco_classes = [line.strip() for line in f.readlines()]\n",
    "print(f\"Loaded {len(coco_classes)} class names\")\n",
    "print(f\"Using model input dimensions: {model_width}x{model_height}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5798bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions for video processing\n",
    "def preprocess_frame(frame, target_size=(model_width, model_height)):\n",
    "    \"\"\"Preprocess frame for Mask R-CNN\"\"\"\n",
    "    # Convert BGR to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Resize image\n",
    "    resized = cv2.resize(rgb_frame, target_size)\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    normalized = resized.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Convert HWC to CHW\n",
    "    chw = np.transpose(normalized, (2, 0, 1))\n",
    "    \n",
    "    # Add batch dimension\n",
    "    batch = np.expand_dims(chw, axis=0)\n",
    "    \n",
    "    return batch\n",
    "\n",
    "def postprocess_detections(outputs, original_shape, conf_threshold=0.5):\n",
    "    \"\"\"Post-process Mask R-CNN outputs\"\"\"\n",
    "    # Extract outputs\n",
    "    boxes = outputs[0]    # Shape: (N, 4)\n",
    "    labels = outputs[1]   # Shape: (N,)\n",
    "    scores = outputs[2]   # Shape: (N,)\n",
    "    masks = outputs[3]    # Shape: (N, 1, H, W)\n",
    "    \n",
    "    # Remove batch dimension if present\n",
    "    if len(boxes.shape) == 3:\n",
    "        boxes = boxes[0]\n",
    "    if len(labels.shape) == 2:\n",
    "        labels = labels[0]\n",
    "    if len(scores.shape) == 2:\n",
    "        scores = scores[0]\n",
    "    if len(masks.shape) == 4 and masks.shape[1] == 1:\n",
    "        masks = masks.squeeze(1)  # Remove channel dimension\n",
    "    \n",
    "    # Filter by confidence threshold\n",
    "    valid_indices = scores > conf_threshold\n",
    "    filtered_boxes = boxes[valid_indices]\n",
    "    filtered_labels = labels[valid_indices]\n",
    "    filtered_scores = scores[valid_indices]\n",
    "    filtered_masks = masks[valid_indices]\n",
    "    \n",
    "    print(f\"Detections after filtering: {len(filtered_scores)}\")\n",
    "    \n",
    "    # Scale boxes to original image size\n",
    "    orig_h, orig_w = original_shape[:2]\n",
    "    if len(filtered_boxes) > 0:\n",
    "        # Scale box coordinates from model dimensions to image dimensions\n",
    "        filtered_boxes[:, [0, 2]] *= orig_w / model_width  # Scale x coordinates\n",
    "        filtered_boxes[:, [1, 3]] *= orig_h / model_height  # Scale y coordinates\n",
    "        \n",
    "        # Scale masks to original image size\n",
    "        if len(filtered_masks) > 0:\n",
    "            scaled_masks = []\n",
    "            for mask in filtered_masks:\n",
    "                mask_resized = cv2.resize(mask, (orig_w, orig_h))\n",
    "                scaled_masks.append(mask_resized)\n",
    "            filtered_masks = np.array(scaled_masks)\n",
    "    \n",
    "    return filtered_boxes, filtered_labels, filtered_scores, filtered_masks\n",
    "\n",
    "def apply_mask_overlay(frame, mask, color=(0, 255, 0), alpha=0.15):\n",
    "    \"\"\"Apply very subtle mask overlay to frame, single color for all masks to preserve video visibility\"\"\"\n",
    "    # Create binary mask (True where mask > 0.5)\n",
    "    mask_binary = mask > 0.5\n",
    "    \n",
    "    # Create colored mask image - using one consistent color \n",
    "    colored_mask = np.zeros_like(frame)\n",
    "    colored_mask[mask_binary] = color\n",
    "    \n",
    "    # Use a very low alpha for much higher transparency\n",
    "    result = cv2.addWeighted(frame, 1 - alpha, colored_mask, alpha, 0)\n",
    "    return result\n",
    "\n",
    "def apply_all_masks_at_once(frame, masks, color=(0, 255, 0), alpha=0.15):\n",
    "    \"\"\"Apply all masks at once to prevent darkening with multiple objects\"\"\"\n",
    "    # Create a single combined mask for all objects\n",
    "    combined_mask = np.zeros((frame.shape[0], frame.shape[1]), dtype=bool)\n",
    "    \n",
    "    # Add each mask to the combined mask\n",
    "    for mask in masks:\n",
    "        if mask.shape == (frame.shape[0], frame.shape[1]):\n",
    "            combined_mask = np.logical_or(combined_mask, mask > 0.5)\n",
    "    \n",
    "    # Create colored mask image\n",
    "    colored_mask = np.zeros_like(frame)\n",
    "    colored_mask[combined_mask] = color\n",
    "    \n",
    "    # Apply the combined mask only once\n",
    "    result = cv2.addWeighted(frame, 1 - alpha, colored_mask, alpha, 0)\n",
    "    return result\n",
    "\n",
    "# Define colors for different classes (BGR format)\n",
    "colors = [\n",
    "    (0, 255, 0),    # Green\n",
    "    (255, 0, 0),    # Blue  \n",
    "    (0, 0, 255),    # Red\n",
    "    (255, 255, 0),  # Cyan\n",
    "    (255, 0, 255),  # Magenta\n",
    "    (0, 255, 255),  # Yellow\n",
    "    (128, 0, 128),  # Purple\n",
    "    (255, 165, 0),  # Orange\n",
    "    (0, 128, 255),  # Light Blue\n",
    "    (128, 128, 0)   # Olive\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f962dd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Mask R-CNN with ONNX Runtime on GPU...\n",
      "Press 'q' to quit the video display\n",
      "Detections after filtering: 6\n",
      "Detections after filtering: 7\n",
      "Detections after filtering: 9\n",
      "Detections after filtering: 5\n",
      "Detections after filtering: 5\n",
      "Detections after filtering: 6\n",
      "Detections after filtering: 7\n",
      "Detections after filtering: 8\n",
      "Detections after filtering: 8\n",
      "Detections after filtering: 7\n",
      "Detections after filtering: 11\n",
      "Detections after filtering: 8\n",
      "Detections after filtering: 6\n",
      "Detections after filtering: 8\n",
      "Detections after filtering: 9\n",
      "Detections after filtering: 8\n",
      "Video processing completed!\n"
     ]
    }
   ],
   "source": [
    "# Process video with Mask R-CNN\n",
    "# You can change the video path and confidence threshold here\n",
    "video_path = \"resources/test_video_street.mp4\"\n",
    "confidence_threshold = 0.5\n",
    "\n",
    "# Visualization options\n",
    "show_masks = True  # Set to False to completely disable masks\n",
    "mask_alpha = 0.2   # Mask transparency (0.2 = 20% opacity)\n",
    "mask_color = (0, 255, 0)  # Single color (green) for all masks\n",
    "\n",
    "# Open video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# FPS calculation variables\n",
    "fps_counter = 0\n",
    "fps_start_time = time.time()\n",
    "fps_display = 0.0\n",
    "\n",
    "# Determine device string for display\n",
    "device_str = \"GPU\" if 'CUDAExecutionProvider' in session.get_providers() else \"CPU\"\n",
    "print(f\"\\nRunning Mask R-CNN with ONNX Runtime on {device_str}...\")\n",
    "print(\"Press 'q' to quit the video display\")\n",
    "\n",
    "# Loop through video frames\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocess frame \n",
    "    input_tensor = preprocess_frame(frame)\n",
    "\n",
    "    # Run inference \n",
    "    try:\n",
    "        outputs = session.run(None, {input_details.name: input_tensor})\n",
    "        \n",
    "        # Post-process \n",
    "        boxes, labels, scores, masks = postprocess_detections(outputs, frame.shape, conf_threshold=confidence_threshold)\n",
    "        \n",
    "        # First, apply all masks at once to prevent darkening with multiple detections\n",
    "        if show_masks and len(masks) > 0:\n",
    "            # Make sure we have valid masks before applying\n",
    "            valid_masks = [mask for mask in masks if mask.shape == (height, width)]\n",
    "            if valid_masks:\n",
    "                frame = apply_all_masks_at_once(frame, valid_masks, mask_color, mask_alpha)\n",
    "        \n",
    "        # Then draw bounding boxes and labels\n",
    "        for i, (box, label, score) in enumerate(zip(boxes, labels, scores)):\n",
    "            if len(box) >= 4:\n",
    "                x1, y1, x2, y2 = map(int, box[:4])\n",
    "                \n",
    "                # Ensure coordinates are within frame bounds\n",
    "                x1, y1 = max(0, x1), max(0, y1)\n",
    "                x2, y2 = min(width, x2), min(height, y2)\n",
    "                \n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), mask_color, 3)\n",
    "                \n",
    "                # Draw label\n",
    "                class_name = coco_classes[int(label)] if int(label) < len(coco_classes) else f\"class_{int(label)}\"\n",
    "                label_text = f\"{class_name}: {score:.2f}\"\n",
    "                label_size = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]\n",
    "                cv2.rectangle(frame, (x1, y1 - label_size[1] - 10), \n",
    "                             (x1 + label_size[0] + 10, y1), mask_color, -1)\n",
    "                cv2.putText(frame, label_text, (x1 + 5, y1 - 5),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Inference error: {e}\")\n",
    "\n",
    "    # Add title at top middle of screen\n",
    "    title_text = \"Mask R-CNN Instance Segmentation\"\n",
    "    title_size = cv2.getTextSize(title_text, cv2.FONT_HERSHEY_SIMPLEX, 1.0, 2)[0]\n",
    "    title_x = (width - title_size[0]) // 2\n",
    "    cv2.rectangle(frame, (title_x - 10, 10), (title_x + title_size[0] + 10, 50), (0, 0, 0), -1)\n",
    "    cv2.putText(frame, title_text, (title_x, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2)\n",
    "\n",
    "    # Calculate and display FPS\n",
    "    fps_counter += 1\n",
    "    if fps_counter % 10 == 0:\n",
    "        fps_end_time = time.time()\n",
    "        fps_display = 10 / (fps_end_time - fps_start_time)\n",
    "        fps_start_time = fps_end_time\n",
    "    \n",
    "    # Draw FPS\n",
    "    fps_text = f\"FPS: {fps_display:.1f} (Mask R-CNN-{device_str})\"\n",
    "    cv2.rectangle(frame, (5, height - 40), (280, height - 10), (0, 0, 0), -1)\n",
    "    cv2.putText(frame, fps_text, (10, height - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "    # Display \n",
    "    cv2.imshow(\"Mask R-CNN ONNX Runtime\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release everything\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Video processing completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onnxruntime-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
