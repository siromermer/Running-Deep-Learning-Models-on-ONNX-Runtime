{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6da5de33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "cuDNN version: 91002\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a42d25d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.22.0\n",
      "GPU\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "\n",
    "print(onnxruntime.__version__)\n",
    "print(onnxruntime.get_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f952f29d",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5ee4c2",
   "metadata": {},
   "source": [
    "# Object Detection with Faster R-CNN and ONNX Runtime GPU\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Export a pre-trained Faster R-CNN model to ONNX format\n",
    "2. Run the model on video using ONNX Runtime with GPU acceleration\n",
    "3. Visualize object detection results in real-time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441d230e",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ada4c91",
   "metadata": {},
   "source": [
    "## 1. Export Faster R-CNN model to ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb68da05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omer/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/omer/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/tmp/ipykernel_6061/853217426.py:16: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n",
      "/home/omer/.local/lib/python3.10/site-packages/torch/nn/functional.py:4705: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  * torch.tensor(scale_factors[i], dtype=torch.float32)\n",
      "/home/omer/.local/lib/python3.10/site-packages/torchvision/ops/boxes.py:174: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  boxes_x = torch.min(boxes_x, torch.tensor(width, dtype=boxes.dtype, device=boxes.device))\n",
      "/home/omer/.local/lib/python3.10/site-packages/torchvision/ops/boxes.py:176: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  boxes_y = torch.min(boxes_y, torch.tensor(height, dtype=boxes.dtype, device=boxes.device))\n",
      "/home/omer/.local/lib/python3.10/site-packages/torch/__init__.py:2174: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert condition, message\n",
      "/home/omer/.local/lib/python3.10/site-packages/torchvision/models/detection/transform.py:308: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(s, dtype=torch.float32, device=boxes.device)\n",
      "/home/omer/.local/lib/python3.10/site-packages/torchvision/models/detection/transform.py:309: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  / torch.tensor(s_orig, dtype=torch.float32, device=boxes.device)\n",
      "/home/omer/.local/lib/python3.10/site-packages/torch/onnx/symbolic_opset9.py:5350: UserWarning: Exporting aten::index operator of advanced indexing in opset 11 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faster R-CNN model exported to models/fasterrcnn.onnx with input size 800x800\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# Define model input dimensions\n",
    "model_width = 800\n",
    "model_height = 800\n",
    "\n",
    "# Load pretrained Faster R-CNN model\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Create dummy input - typical input size for Faster R-CNN\n",
    "dummy_input = torch.randn(1, 3, model_height, model_width)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    dummy_input, \n",
    "    \"models/fasterrcnn.onnx\",\n",
    "    opset_version=11,\n",
    "    input_names=[\"images\"],\n",
    "    output_names=[\"boxes\", \"labels\", \"scores\"],\n",
    "    dynamic_axes={\n",
    "        \"images\": {0: \"batch_size\"},\n",
    "        \"boxes\": {0: \"num_detections\"},\n",
    "        \"labels\": {0: \"num_detections\"},\n",
    "        \"scores\": {0: \"num_detections\"}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Faster R-CNN model exported to models/fasterrcnn.onnx with input size {model_width}x{model_height}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de1b720",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712239c7",
   "metadata": {},
   "source": [
    "## 2. Run Faster R-CNN model on video with ONNX Runtime GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9247770e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available ONNX Runtime providers: ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "CUDA provider available: True\n",
      "Session providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Input name: images\n",
      "Output 0: boxes, shape: ['num_detections', 4]\n",
      "Output 1: labels, shape: ['num_detections']\n",
      "Output 2: scores, shape: ['num_detections']\n",
      "Loaded 81 class names\n",
      "Using model input dimensions: 800x800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-08-24 16:20:49.851928577 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-08-24 16:20:49.851958398 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Check available providers first\n",
    "print(\"Available ONNX Runtime providers:\", ort.get_available_providers())\n",
    "print(\"CUDA provider available:\", 'CUDAExecutionProvider' in ort.get_available_providers())\n",
    "\n",
    "# Load pretrained Faster R-CNN model\n",
    "session = ort.InferenceSession(\"models/fasterrcnn.onnx\", providers=['CUDAExecutionProvider'])\n",
    "print(\"Session providers:\", session.get_providers())\n",
    "\n",
    "# Get input details\n",
    "input_details = session.get_inputs()[0]\n",
    "print(f\"Input name: {input_details.name}\")\n",
    "\n",
    "# Check output shapes\n",
    "for i, output in enumerate(session.get_outputs()):\n",
    "    print(f\"Output {i}: {output.name}, shape: {output.shape}\")\n",
    "\n",
    "# Load COCO class names from file\n",
    "with open('resources/coco_labels_rcnn.txt', 'r') as f:\n",
    "    coco_classes = [line.strip() for line in f.readlines()]\n",
    "print(f\"Loaded {len(coco_classes)} class names\")\n",
    "print(f\"Using model input dimensions: {model_width}x{model_height}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b818114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame, target_size=(model_width, model_height)):\n",
    "    \"\"\"Preprocess frame for Faster R-CNN\"\"\"\n",
    "    # Convert BGR to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Resize image\n",
    "    resized = cv2.resize(rgb_frame, target_size)\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    normalized = resized.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Convert HWC to CHW\n",
    "    chw = np.transpose(normalized, (2, 0, 1))\n",
    "    \n",
    "    # Add batch dimension\n",
    "    batch = np.expand_dims(chw, axis=0)\n",
    "    \n",
    "    return batch\n",
    "\n",
    "def postprocess_detections(outputs, original_shape, conf_threshold=0.5):\n",
    "    \"\"\"Post-process Faster R-CNN outputs\"\"\"\n",
    "    # Faster R-CNN typically outputs: boxes, labels, scores\n",
    "    # The exact order depends on how the model was exported\n",
    "    if len(outputs) == 3:\n",
    "        boxes, labels, scores = outputs\n",
    "    else:\n",
    "        # If single output, it might be a dictionary-like structure\n",
    "        # We'll need to adapt based on actual output format\n",
    "        print(f\"Unexpected number of outputs: {len(outputs)}\")\n",
    "        return [], [], []\n",
    "    \n",
    "    # Remove batch dimension if present\n",
    "    if len(boxes.shape) == 3:\n",
    "        boxes = boxes[0]\n",
    "    if len(labels.shape) == 2:\n",
    "        labels = labels[0]\n",
    "    if len(scores.shape) == 2:\n",
    "        scores = scores[0]\n",
    "    \n",
    "    # Filter by confidence threshold\n",
    "    valid_indices = scores > conf_threshold\n",
    "    filtered_boxes = boxes[valid_indices]\n",
    "    filtered_labels = labels[valid_indices]\n",
    "    filtered_scores = scores[valid_indices]\n",
    "    \n",
    "    # Scale boxes to original image size\n",
    "    orig_h, orig_w = original_shape[:2]\n",
    "    if len(filtered_boxes) > 0:\n",
    "        # Scale box coordinates from model dimensions to image dimensions\n",
    "        filtered_boxes[:, [0, 2]] *= orig_w / model_width  # Scale x coordinates\n",
    "        filtered_boxes[:, [1, 3]] *= orig_h / model_height  # Scale y coordinates\n",
    "    \n",
    "    return filtered_boxes, filtered_labels, filtered_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ea15983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Faster R-CNN with ONNX Runtime on GPU...\n",
      "Press 'q' to quit the video display\n",
      "Video processing completed!\n"
     ]
    }
   ],
   "source": [
    "# Process video with Faster R-CNN\n",
    "# You can change the video path and confidence threshold here\n",
    "video_path = \"resources/test_video_street.mp4\"\n",
    "confidence_threshold = 0.5\n",
    "\n",
    "# Open video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# FPS calculation variables\n",
    "fps_counter = 0\n",
    "fps_start_time = time.time()\n",
    "fps_display = 0.0\n",
    "\n",
    "# Determine device string for display\n",
    "device_str = \"GPU\" if 'CUDAExecutionProvider' in session.get_providers() else \"CPU\"\n",
    "print(f\"\\nRunning Faster R-CNN with ONNX Runtime on {device_str}...\")\n",
    "print(\"Press 'q' to quit the video display\")\n",
    "\n",
    "# Loop through video frames\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocess frame \n",
    "    input_tensor = preprocess_frame(frame)\n",
    "\n",
    "    # Run inference \n",
    "    try:\n",
    "        outputs = session.run(None, {input_details.name: input_tensor})\n",
    "        \n",
    "        # Post-process \n",
    "        boxes, labels, scores = postprocess_detections(outputs, frame.shape, conf_threshold=confidence_threshold)\n",
    "        \n",
    "        # Draw detections \n",
    "        for box, label, score in zip(boxes, labels, scores):\n",
    "            if len(box) >= 4:\n",
    "                x1, y1, x2, y2 = map(int, box[:4])\n",
    "                \n",
    "                # Ensure coordinates are within frame bounds\n",
    "                x1, y1 = max(0, x1), max(0, y1)\n",
    "                x2, y2 = min(width, x2), min(height, y2)\n",
    "                \n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                \n",
    "                # Draw label\n",
    "                class_name = coco_classes[int(label)] if int(label) < len(coco_classes) else f\"class_{int(label)}\"\n",
    "                label_text = f\"{class_name}: {score:.2f}\"\n",
    "                label_size = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]\n",
    "                cv2.rectangle(frame, (x1, y1 - label_size[1] - 10), \n",
    "                             (x1 + label_size[0], y1), (0, 255, 0), -1)\n",
    "                cv2.putText(frame, label_text, (x1, y1 - 5),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Inference error: {e}\")\n",
    "\n",
    "    # Add title at top middle of screen\n",
    "    title_text = \"Faster R-CNN Object Detection\"\n",
    "    title_size = cv2.getTextSize(title_text, cv2.FONT_HERSHEY_SIMPLEX, 1.0, 2)[0]\n",
    "    title_x = (width - title_size[0]) // 2\n",
    "    cv2.rectangle(frame, (title_x - 10, 10), (title_x + title_size[0] + 10, 50), (0, 0, 0), -1)\n",
    "    cv2.putText(frame, title_text, (title_x, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2)\n",
    "\n",
    "    # Calculate and display FPS\n",
    "    fps_counter += 1\n",
    "    if fps_counter % 10 == 0:\n",
    "        fps_end_time = time.time()\n",
    "        fps_display = 10 / (fps_end_time - fps_start_time)\n",
    "        fps_start_time = fps_end_time\n",
    "    \n",
    "    # Draw FPS\n",
    "    fps_text = f\"FPS: {fps_display:.1f} (Faster R-CNN-{device_str})\"\n",
    "    cv2.rectangle(frame, (5, height - 40), (280, height - 10), (0, 0, 0), -1)\n",
    "    cv2.putText(frame, fps_text, (10, height - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "    # Display \n",
    "    cv2.imshow(\"Faster R-CNN ONNX Runtime\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release everything\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Video processing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5971c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onnxruntime-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
